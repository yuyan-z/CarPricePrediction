import numpy as np
import pandas as pd
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader

# ======================
# 1. 配置参数
# ======================
CSV_PATH = "data.csv"
VALUE_COL = "value"      # 目标列名
SEQ_LEN = 12             # 用过去多少个周作为输入（比如 12 周）
TRAIN_RATIO = 0.8        # 按时间切分 train/test
BATCH_SIZE = 32
NUM_EPOCHS = 50
LR = 1e-3
HIDDEN_SIZE = 64
NUM_LAYERS = 1

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# ======================
# 2. 读取 & 预处理数据
# ======================
df = pd.read_csv(CSV_PATH)

# 确保按时间排序（如果有 date 列）
if "date" in df.columns:
    df["date"] = pd.to_datetime(df["date"])
    df = df.sort_values("date")

# 取出目标序列，转为 float32 的 numpy 数组
series = df[VALUE_COL].values.astype(np.float32)

# 简单标准化：减均值除以标准差（也可以换成 MinMax）
mean = series.mean()
std = series.std() if series.std() > 0 else 1.0
series_norm = (series - mean) / std


# ======================
# 3. 构造监督学习样本: 用前 SEQ_LEN 周预测下一周
# ======================
def create_sequences(data: np.ndarray, seq_len: int):
    """
    data: 形状 (T,)
    返回:
        X: (num_samples, seq_len, 1)
        y: (num_samples, 1)
    """
    xs, ys = [], []
    for i in range(len(data) - seq_len):
        x = data[i : i + seq_len]       # 长度 seq_len
        y = data[i + seq_len]           # 下一个点
        xs.append(x)
        ys.append(y)
    X = np.array(xs)  # (N, seq_len)
    y = np.array(ys)  # (N,)
    # 增加 feature 维度 -> (N, seq_len, 1)
    X = X[..., np.newaxis]
    y = y[..., np.newaxis]
    return X.astype(np.float32), y.astype(np.float32)


X, y = create_sequences(series_norm, SEQ_LEN)
print("X shape:", X.shape)  # (N, seq_len, 1)
print("y shape:", y.shape)  # (N, 1)

# 按时间切分 train / test（前 80% train，后 20% test）
num_samples = X.shape[0]
train_size = int(num_samples * TRAIN_RATIO)

X_train = X[:train_size]
y_train = y[:train_size]
X_test = X[train_size:]
y_test = y[train_size:]


# ======================
# 4. 定义 PyTorch Dataset
# ======================
class TimeSeriesDataset(Dataset):
    def __init__(self, X: np.ndarray, y: np.ndarray):
        self.X = torch.from_numpy(X)  # (N, seq_len, 1)
        self.y = torch.from_numpy(y)  # (N, 1)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        # 返回 (seq_len, 1) 和 (1,)
        return self.X[idx], self.y[idx]


train_dataset = TimeSeriesDataset(X_train, y_train)
test_dataset = TimeSeriesDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)


# ======================
# 5. 定义 LSTM 模型
# ======================
class LSTMModel(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=1, output_size=1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,   # 输入 (batch, seq_len, input_size)
        )
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # x: (batch, seq_len, input_size)
        batch_size = x.size(0)

        # 初始化 hidden state & cell state
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)

        # out: (batch, seq_len, hidden_size)
        out, _ = self.lstm(x, (h0, c0))

        # 只取最后一个时间步的输出
        last_out = out[:, -1, :]        # (batch, hidden_size)

        # 映射到输出维度
        y_hat = self.fc(last_out)       # (batch, output_size)
        return y_hat


model = LSTMModel(
    input_size=1,
    hidden_size=HIDDEN_SIZE,
    num_layers=NUM_LAYERS,
    output_size=1,
).to(device)

criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=LR)


# ======================
# 6. 训练循环
# ======================
for epoch in range(1, NUM_EPOCHS + 1):
    model.train()
    train_losses = []

    for batch_x, batch_y in train_loader:
        batch_x = batch_x.to(device)  # (B, seq_len, 1)
        batch_y = batch_y.to(device)  # (B, 1)

        optimizer.zero_grad()
        outputs = model(batch_x)      # (B, 1)

        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()

        train_losses.append(loss.item())

    # 每轮结束后在 test 集上评估一下
    model.eval()
    test_losses = []
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            test_losses.append(loss.item())

    print(
        f"Epoch [{epoch}/{NUM_EPOCHS}] "
        f"Train Loss: {np.mean(train_losses):.4f} "
        f"Test Loss: {np.mean(test_losses):.4f}"
    )


# ======================
# 7. 简单做几个预测看看（可选）
# ======================
model.eval()
with torch.no_grad():
    # 取最后一个 test 样本
    sample_x = torch.from_numpy(X_test[-1:]).to(device)  # shape (1, seq_len, 1)
    pred_norm = model(sample_x).cpu().numpy()[0, 0]

    # 反标准化
    pred_value = pred_norm * std + mean
    true_value = y_test[-1, 0] * std + mean

    print("Last test true value:", true_value)
    print("Last test pred value:", pred_value)
