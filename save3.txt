import numpy as np
import pandas as pd
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader

# ======================
# 1. 配置参数
# ======================
CSV_PATH = "data.csv"
TARGET_COL = "value"     # 要预测的目标列名（比如借贷利率）

SEQ_LEN = 12             # 用过去多少个周作为输入（比如 12 周）
TRAIN_RATIO = 0.8        # 按时间切分 train/test
BATCH_SIZE = 32
NUM_EPOCHS = 50
LR = 1e-3
HIDDEN_SIZE = 64
NUM_LAYERS = 1

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# ======================
# 2. 读取 & 预处理数据
# ======================
df = pd.read_csv(CSV_PATH)

# 确保按时间排序（如果有 date 列）
if "date" in df.columns:
    df["date"] = pd.to_datetime(df["date"])
    df = df.sort_values("date")

# 选出数值型列（作为候选特征）
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

if TARGET_COL not in numeric_cols:
    raise ValueError(f"TARGET_COL '{TARGET_COL}' 不在数值列中，请检查 data.csv 或修改 TARGET_COL")

# 特征列：这里简单起见，用所有数值列作为输入特征（包含目标列的历史值）
feature_cols = numeric_cols
print("Feature columns:", feature_cols)
print("Target column:", TARGET_COL)

# 构造特征矩阵 X_all: (T, num_features)
X_all = df[feature_cols].values.astype(np.float32)
num_features = X_all.shape[1]

# 目标序列 y_all: (T,)
y_all = df[TARGET_COL].values.astype(np.float32)

# 对特征做列级标准化（每一列减均值/除标准差）
feat_mean = X_all.mean(axis=0)
feat_std = X_all.std(axis=0)
feat_std[feat_std == 0] = 1.0  # 避免除以 0

X_norm = (X_all - feat_mean) / feat_std

# 对目标单独做标准化（方便之后反标准化看真实值）
y_mean = y_all.mean()
y_std = y_all.std() if y_all.std() > 0 else 1.0
y_norm = (y_all - y_mean) / y_std


# ======================
# 3. 构造监督学习样本: 用前 SEQ_LEN 周预测下一周
# ======================
def create_sequences_multivar(X: np.ndarray, y: np.ndarray, seq_len: int):
    """
    X: (T, num_features)
    y: (T,)
    返回:
        X_seq: (N, seq_len, num_features)
        y_seq: (N, 1)
    """
    xs, ys = [], []
    T = len(y)
    for i in range(T - seq_len):
        x_i = X[i : i + seq_len, :]    # (seq_len, num_features)
        y_i = y[i + seq_len]           # 标量
        xs.append(x_i)
        ys.append(y_i)
    X_seq = np.array(xs, dtype=np.float32)          # (N, seq_len, num_features)
    y_seq = np.array(ys, dtype=np.float32)[:, None] # (N, 1)
    return X_seq, y_seq


X, y = create_sequences_multivar(X_norm, y_norm, SEQ_LEN)
print("X shape:", X.shape)  # (N, seq_len, num_features)
print("y shape:", y.shape)  # (N, 1)

# 按时间切分 train / test
num_samples = X.shape[0]
train_size = int(num_samples * TRAIN_RATIO)

X_train = X[:train_size]
y_train = y[:train_size]
X_test = X[train_size:]
y_test = y[train_size:]


# ======================
# 4. 定义 PyTorch Dataset
# ======================
class TimeSeriesDataset(Dataset):
    def __init__(self, X: np.ndarray, y: np.ndarray):
        self.X = torch.from_numpy(X)  # (N, seq_len, num_features)
        self.y = torch.from_numpy(y)  # (N, 1)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


train_dataset = TimeSeriesDataset(X_train, y_train)
test_dataset = TimeSeriesDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)


# ======================
# 5. 定义多变量 LSTM 模型（单步预测）
# ======================
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=1, output_size=1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,   # 输入 (batch, seq_len, input_size)
        )
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # x: (batch, seq_len, input_size)
        batch_size = x.size(0)

        # 初始化 hidden state & cell state
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)

        # out: (batch, seq_len, hidden_size)
        out, _ = self.lstm(x, (h0, c0))

        # 只取最后一个时间步的输出
        last_out = out[:, -1, :]        # (batch, hidden_size)

        # 映射到输出维度 (1)
        y_hat = self.fc(last_out)       # (batch, 1)
        return y_hat


model = LSTMModel(
    input_size=num_features,           # ⭐ 多变量输入
    hidden_size=HIDDEN_SIZE,
    num_layers=NUM_LAYERS,
    output_size=1,                     # 单步预测
).to(device)

criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=LR)


# ======================
# 6. 训练循环
# ======================
for epoch in range(1, NUM_EPOCHS + 1):
    model.train()
    train_losses = []

    for batch_x, batch_y in train_loader:
        batch_x = batch_x.to(device)  # (B, seq_len, num_features)
        batch_y = batch_y.to(device)  # (B, 1)

        optimizer.zero_grad()
        outputs = model(batch_x)      # (B, 1)

        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()

        train_losses.append(loss.item())

    # 每轮结束后在 test 集上评估一下
    model.eval()
    test_losses = []
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            test_losses.append(loss.item())

    print(
        f"Epoch [{epoch}/{NUM_EPOCHS}] "
        f"Train Loss: {np.mean(train_losses):.4f} "
        f"Test Loss: {np.mean(test_losses):.4f}"
    )


# ======================
# 7. 做一个单步预测示例（可选）
# ======================
model.eval()
with torch.no_grad():
    # 取最后一个 test 样本
    sample_x = torch.from_numpy(X_test[-1:]).to(device)  # shape (1, seq_len, num_features)
    pred_norm = model(sample_x).cpu().numpy()[0, 0]

    # 反标准化
    pred_value = pred_norm * y_std + y_mean
    true_value = y_test[-1, 0] * y_std + y_mean

    print("Last test true target:", true_value)
    print("Last test pred target:", pred_value)
